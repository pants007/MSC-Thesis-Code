{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import download_dataset, generate_taxonomy_dataframe, BeetleSet, get_dataloaders, get_class_freqs_dict, generate_labels_dict\n",
    "from loss_functions import WeightedCCELoss, WeightedBCELoss\n",
    "from train import weighted_fit_tb_recon, weighted_test_recon, weighted_fit_tb, weighted_test\n",
    "from display import one_shot\n",
    "import fusion_model_dictionaries as fmd\n",
    "import models\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, RandomVerticalFlip, RandomRotation, Normalize, ToTensor\n",
    "import torchvision\n",
    "from transforms import RandomResizedCrop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define new pipeline including reconstruction tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, transforms, seed, batch_size,\n",
    "                                num_workers, device, num_epochs, loss_coefs=None, root='../output/msc-thesis-22/models/', criterion_constructor=WeightedCCELoss, early_stopping=None, dataloader_seed=None, \n",
    "                                optimizer_constructor = torch.optim.Adam,  aux_optimizer_args = {}, \n",
    "                                scheduler_constructor = torch.optim.lr_scheduler.ExponentialLR, \n",
    "                                aux_scheduler_args = {'gamma': 0.995}, reduction='mean', weight_scheme=None, \n",
    "                                aux_criterion_args={}, use_amp = True, use_scaler=True, pin_memory = True, \n",
    "                                grad_clip_val = 10**4, load_state_dict=False, tb_root = '../output/msc-thesis-22/tensorboard/',\n",
    "                                disc_constructor = None, aux_disc_args = {}, optim_d_constructor = torch.optim.Adam,\n",
    "                                aux_optim_d_args={}, optim_d_scheduler_constructor=torch.optim.lr_scheduler.ExponentialLR, \n",
    "                                aux_optim_d_scheduler_args={'gamma': 0.995}, disc_loss_threshold = 0.1, scale_factor = None):\n",
    "\n",
    "\n",
    "    tmap = {'subfamily': '0', 'tribe': '1', 'genus': '2', 'species': '3'}\n",
    "    tags = model_name.split(' ')\n",
    "    model_name_ext = tags[0] + '_' + \\\n",
    "        ''.join([tmap[taxon] for taxon in taxa])\n",
    "    tags[0] = model_name_ext\n",
    "    model_name_ext = ' '.join(tags)\n",
    "    \n",
    "    # inference branch\n",
    "    if load_state_dict:\n",
    "        dataframe = generate_taxonomy_dataframe(root + model_name_ext + '/taxonomy-modified.csv')\n",
    "        labels_dict = generate_labels_dict(dataframe, taxa)\n",
    "        classes_per_taxon = [len(labels_dict[taxon]) for taxon in taxa]\n",
    "        constructor_args = {**aux_model_args, **\n",
    "                                {'classes_per_taxon': classes_per_taxon}}    \n",
    "        model = model_constructor(**constructor_args).to(device)\n",
    "        state = torch.load(root + model_name_ext + '/state_dict.pt')\n",
    "        model.load_state_dict(state)\n",
    "        return model\n",
    "    \n",
    "    # training branch\n",
    "         \n",
    "    else:\n",
    "        download_dataset()\n",
    "        Path(root + model_name_ext).mkdir(parents=True, exist_ok=True)\n",
    "        generate_taxonomy_dataframe('data/beetles/taxonomy.csv', root + model_name_ext +\n",
    "                                    '/taxonomy-modified.csv', drop_min=9)\n",
    "        dataset = BeetleSet(csv_path=root + model_name_ext +\n",
    "                            '/taxonomy-modified.csv', taxa=taxa)\n",
    "        classes_per_taxon = [len(dataset.labels_dict[taxon]) for taxon in taxa]\n",
    "        dataloaders, dataset_sizes = get_dataloaders(dataset, 0.2, transforms, None, batch_size,\n",
    "                                                    num_workers, seed, dataloader_seed, pin_memory)\n",
    "        if weight_scheme == 'icf':\n",
    "            train_idxs = dataloaders['train'].dataset.dataset.indices\n",
    "            class_freqs_dict = get_class_freqs_dict(\n",
    "                taxa, dataset.dataframe, train_idxs)\n",
    "            class_freqs = []\n",
    "            for taxon in taxa:\n",
    "                keys, vals = zip(*class_freqs_dict[taxon].items())\n",
    "                assert list(keys) == list(dataset.labels_dict[taxon].keys())\n",
    "                class_freqs.append(torch.tensor(list(vals)))\n",
    "            weights_per_taxon = [1.0/taxon_freqs for taxon_freqs in class_freqs]\n",
    "        else:\n",
    "            weights_per_taxon = None\n",
    "\n",
    "        if weights_per_taxon is not None:\n",
    "            assert len(weights_per_taxon) == len(taxa)\n",
    "            for i, taxon in enumerate(taxa):\n",
    "                assert torch.is_tensor(weights_per_taxon[i])\n",
    "                assert len(weights_per_taxon[i].shape) == 1\n",
    "                assert len(weights_per_taxon[i]) == len(dataset.labels_dict[taxon])\n",
    "\n",
    "        if loss_coefs is not None:\n",
    "            assert torch.is_tensor(loss_coefs)\n",
    "            assert len(loss_coefs.shape) == 1\n",
    "            if scale_factor is None:\n",
    "                assert len(loss_coefs) == len(taxa)\n",
    "            elif disc_constructor is None:\n",
    "                assert len(loss_coefs) == len(taxa) + 1\n",
    "            else:\n",
    "                assert len(loss_coefs) == len(taxa) + 2\n",
    "        else:\n",
    "            if scale_factor is None:\n",
    "                loss_coefs= torch.ones(len(taxa))\n",
    "            elif disc_constructor is None:\n",
    "                loss_coefs = torch.ones(len(taxa) + 1)\n",
    "            else:\n",
    "                loss_coefs = torch.ones(len(taxa) + 2)\n",
    "\n",
    "        constructor_args = {**aux_model_args, **\n",
    "                            {'classes_per_taxon': classes_per_taxon}}\n",
    "        model = model_constructor(**constructor_args).to(device)\n",
    "\n",
    "        criterion_args = {**aux_criterion_args, **\n",
    "                        {'weights_per_taxon': weights_per_taxon}}\n",
    "        criterion = criterion_constructor(**criterion_args)\n",
    "        \n",
    "        optimizer_args = {**aux_optimizer_args, **\n",
    "                        {'params': model.parameters()}}\n",
    "        optimizer = optimizer_constructor(**optimizer_args)\n",
    "        \n",
    "        scheduler_args = {**aux_scheduler_args, **\n",
    "                        {'optimizer': optimizer}}\n",
    "        \n",
    "        scheduler = scheduler_constructor(**scheduler_args)\n",
    "        \n",
    "        disc = None\n",
    "        optim_d = None\n",
    "        optim_d_scheduler = None\n",
    "        if disc_constructor is not None:\n",
    "            disc_args = {**aux_disc_args}\n",
    "            disc = disc_constructor(**disc_args).to(device)\n",
    "            optim_d_args = {**aux_optim_d_args, **\n",
    "                            {'params': disc.parameters()}}\n",
    "            optim_d = optim_d_constructor(**optim_d_args)\n",
    "            optim_d_scheduler_args = {**aux_optim_d_scheduler_args, **\n",
    "                                    {'optimizer': optim_d}}\n",
    "            optim_d_scheduler = optim_d_scheduler_constructor(**optim_d_scheduler_args)\n",
    "        \n",
    "        if __name__ == '__main__':\n",
    "            if scale_factor is not None:\n",
    "                model, metrics = weighted_fit_tb_recon( model, criterion, optimizer, scheduler, dataloaders, taxa, dataset_sizes,\n",
    "                                                        device, loss_coefs, num_epochs, early_stopping=early_stopping, reduction=reduction, use_amp=use_amp, use_scaler=use_scaler, grad_clip_val = grad_clip_val, \n",
    "                                                        tb_path=tb_root, tags=tags, disc=disc, optim_d=optim_d, \n",
    "                                                        optim_d_scheduler = optim_d_scheduler,  disc_loss_threshold=disc_loss_threshold,\n",
    "                                                        scale_factor=scale_factor)\n",
    "            else:\n",
    "                model, metrics = weighted_fit_tb(    model, criterion, optimizer, scheduler, dataloaders, taxa, dataset_sizes, \n",
    "                                    device, loss_coefs, num_epochs, early_stopping=early_stopping, reduction=reduction, use_amp=use_amp, use_scaler=use_scaler, grad_clip_val=grad_clip_val,\n",
    "                                    tb_path=tb_root, tags=tags)\n",
    "\n",
    "        \n",
    "        torch.save(model.state_dict(), root + model_name_ext + '/state_dict.pt')\n",
    "        criterion.weights_per_taxon = None\n",
    "        \n",
    "        if scale_factor is not None:\n",
    "            test_metrics = weighted_test_recon(model, criterion, dataloaders['test'], device,\n",
    "                                        dataset_sizes, taxa, tb_root, tags, disc=disc, scale_factor=scale_factor)\n",
    "        else:\n",
    "            \n",
    "            test_metrics = weighted_test(model, criterion, dataloaders['test'], device, dataset_sizes, taxa)\n",
    "\n",
    "        for taxon in taxa + ['total']:\n",
    "            metrics[taxon]['test'] = test_metrics[taxon]\n",
    "        if scale_factor is not None:\n",
    "            metrics['test_recon_loss'] = test_metrics['recon_loss']\n",
    "            metrics['test_adv_loss'] = test_metrics['adv_loss']\n",
    "            metrics['test_disc_loss'] = test_metrics['disc_loss']\n",
    "\n",
    "        metrics['model_type'] = str(type(model))\n",
    "        metrics['classes_per_taxon'] = classes_per_taxon\n",
    "        metrics['aux_model_args'] = {}\n",
    "\n",
    "        for name, arg in aux_model_args.items():\n",
    "            if name == 'hidden_features_per_taxon':\n",
    "                metrics['aux_model_args'][name] = arg\n",
    "            elif name == 'model':\n",
    "                metrics['aux_model_args'][name] = str(type(arg))\n",
    "            elif type(arg) == dict:\n",
    "                metrics['aux_model_args'][name] = arg\n",
    "            else:\n",
    "                metrics['aux_model_args'][name] = str(arg)\n",
    "\n",
    "        metrics['criterion_type'] = str(type(criterion))\n",
    "        metrics['reduction'] = reduction\n",
    "        \n",
    "        metrics['weight_scheme'] = weight_scheme\n",
    "        if weights_per_taxon is None:\n",
    "            metrics['weights_per_taxon'] = weights_per_taxon\n",
    "        else:\n",
    "            metrics['weights_per_taxon'] = [tensor.tolist()\n",
    "                                            for tensor in weights_per_taxon]\n",
    "        \n",
    "        metrics['aux_criterion_args'] = {}   \n",
    "        for name, arg in aux_criterion_args.items():\n",
    "            metrics['aux_criterion_args'][name] = arg\n",
    "        \n",
    "        metrics['optimizer_type'] = str(type(optimizer))\n",
    "        metrics['aux_optimizer_args'] = {}\n",
    "        for name, arg in aux_optimizer_args.items():\n",
    "            metrics['aux_optimizer_args'][name] = arg\n",
    "        \n",
    "        metrics['scheduler_type'] = str(type(scheduler))\n",
    "        metrics['aux_scheduler_args'] = {}\n",
    "        for name, arg in aux_scheduler_args.items():\n",
    "            if name == 'lr_lambda' :\n",
    "                metrics['aux_scheduler_args'][name] = str(arg)\n",
    "            else:       \n",
    "                metrics['aux_scheduler_args'][name] = arg\n",
    "        #TODO we could probably simplify the boiler plate code below as it basically a copy of what is done above       \n",
    "        if optim_d is not None:\n",
    "            metrics['optim_d_type'] = str(type(optim_d))\n",
    "            metrics['aux_optim_d_args'] = {}\n",
    "            for name, arg in aux_optim_d_args.items():\n",
    "                metrics['aux_optim_d_args'][name] = arg\n",
    "            \n",
    "            metrics['optim_d_scheduler_type'] = str(type(optim_d_scheduler))\n",
    "            metrics['aux_optim_d_scheduler_args'] = {}\n",
    "            for name, arg in aux_optim_d_scheduler_args.items():\n",
    "                if name == 'lr_lambda' :\n",
    "                    metrics['aux_optim_d_scheduler_args'] = str(arg)\n",
    "                else:\n",
    "                    metrics['aux_optim_d_scheduler_args'] = arg\n",
    "\n",
    "        metrics['batch_size'] = batch_size\n",
    "\n",
    "        metrics['seed'] = seed\n",
    "\n",
    "        with open(root + model_name_ext + '/metrics.json', 'w') as file:\n",
    "            json.dump(metrics, file, indent=4)\n",
    "\n",
    "        return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_root = '../output/msc-thesis-22/models/'\n",
    "baseline_root = models_root + 'baseline/'\n",
    "extensions_root = models_root + 'extensions/'\n",
    "custom_root = models_root + 'custom/'\n",
    "recon_root = models_root + 'recon/'\n",
    "relics_root = models_root + 'relics/'\n",
    "taxa = ['subfamily', 'tribe', 'genus', 'species']\n",
    "taxa_rev = taxa[::-1]\n",
    "BEETLENET_MEAN = np.array(\n",
    "    [0.8442649, 0.82529384, 0.82333773], dtype=np.float32)\n",
    "BEETLENET_STD = np.array([0.28980458, 0.32252666, 0.3240354], dtype=np.float32)\n",
    "BEETLENET_AVERAGE_SHAPE = (224, 448)\n",
    "default_transforms = [Compose([Resize(BEETLENET_AVERAGE_SHAPE), ToTensor(\n",
    "), Normalize(BEETLENET_MEAN, BEETLENET_STD)])] * 3\n",
    "batch_size = 64\n",
    "num_workers = 6\n",
    "num_epochs = 400\n",
    "seed = 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "train_transforms = Compose([\n",
    "    RandomVerticalFlip(p=0.5),\n",
    "    RandomRotation((-3, 3), fill=255, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    RandomResizedCrop(min_scale=0.95, max_scale=1),\n",
    "    Resize(BEETLENET_AVERAGE_SHAPE),\n",
    "    ToTensor(),\n",
    "    Normalize(BEETLENET_MEAN, BEETLENET_STD)])\n",
    "modified_transforms = [train_transforms] + default_transforms[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_global(seed: int):\n",
    "    #os.environ['PYTHONHASHSEED'] = str(seed) i am not sure about this one\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    #force pytorch to use deterministic algorithms for all operations when available,\n",
    "    # and throw and error when operations cannot be executed deterministically.\n",
    "    #torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def reset_seed(seed, resave=True):\n",
    "    seed_global(seed)\n",
    "    base_model = torchvision.models.resnet18(False)\n",
    "    if os.path.exists(models_root + 'init_weights.pt') and not(resave):\n",
    "        init_weights = torch.load(models_root + 'init_weights.pt')\n",
    "        base_model.load_state_dict(init_weights)\n",
    "    else:\n",
    "        init_weights = base_model.state_dict()\n",
    "        torch.save(init_weights, models_root + 'init_weights.pt')\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain forwards/sideways models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forward_cce_cpo():\n",
    "    model_name = 'forward cce cpo'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_forward_graph_dict('conv_per_output')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedCCELoss, reduction='mean', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def train_forward_cce_conv():\n",
    "    model_name = 'forward cce conv'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_forward_graph_dict('conv')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedCCELoss, reduction='mean', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "def train_forward_cce_add():\n",
    "    model_name = 'forward cce add'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_forward_graph_dict('add')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedCCELoss, reduction='mean', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train_forward_bce_cpo():\n",
    "    model_name = 'forward bce cpo'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_forward_graph_dict('conv_per_output')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedBCELoss, reduction='sum', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_forward_bce_conv():\n",
    "    model_name = 'forward bce conv'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_forward_graph_dict('conv')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedBCELoss, reduction='sum', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_forward_bce_add():\n",
    "    model_name = 'forward bce add'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_forward_graph_dict('add')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedBCELoss, reduction='sum', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_backward_cce_cpo():\n",
    "    model_name = 'backward cce cpo'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_backward_graph_dict('conv_per_output')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedCCELoss, reduction='mean', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_backward_cce_conv():\n",
    "    model_name = 'backward cce conv'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_backward_graph_dict('conv')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedCCELoss, reduction='mean', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_backward_cce_add():\n",
    "    model_name = 'backward cce add'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_backward_graph_dict('add')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedCCELoss, reduction='mean', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_backward_bce_cpo():\n",
    "    model_name = 'backward bce cpo'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_backward_graph_dict('conv_per_output')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedBCELoss, reduction='sum', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_backward_bce_conv():\n",
    "    model_name = 'backward bce conv'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_backward_graph_dict('conv')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedBCELoss, reduction='sum', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_backward_bce_add():\n",
    "    model_name = 'backward bce add'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_backward_graph_dict('add')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedBCELoss, reduction='sum', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def train_sideways_bce_cpo():\n",
    "    model_name = 'sideways bce cpo'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_sideways_graph_dict('conv_per_output')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, num_epochs, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedBCELoss, reduction='sum', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    " \n",
    "#jens\n",
    "# train_backward_cce_conv()\n",
    "# train_backward_bce_conv()\n",
    "# train_forward_cce_conv()\n",
    "# train_forward_bce_conv()\n",
    "# train_backward_cce_add()\n",
    "# train_backward_bce_add()\n",
    "\n",
    "#mathias\n",
    "# train_forward_cce_cpo()\n",
    "# train_backward_cce_cpo()\n",
    "# train_backward_bce_cpo()\n",
    "# train_forward_bce_cpo()\n",
    "# train_forward_cce_add()\n",
    "# train_forward_bce_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 0m 33s\n",
      "----------\n",
      "Training stats:\n",
      "\n",
      "taxon:         subfamily    tribe        genus        species      total        \n",
      "loss:          0.3961       0.9685       10.6432      42.6759      13.6709      \n",
      "acc:           0.9108       0.8547       0.4487       0.0668       0.5703       \n",
      "best loss:     0.3961       0.9685       10.6432      42.6759      13.6709      \n",
      "best acc:      0.9108       0.8547       0.4487       0.0668       0.5703       \n",
      "\n",
      "total loss    13.6709       13.6709      \n",
      "\n",
      "Validation stats:\n",
      "\n",
      "taxon:         subfamily    tribe        genus        species      total        \n",
      "loss:          0.4319       1.2172       5.9241       6.7328       3.5765       \n",
      "acc:           0.9046       0.8915       0.2834       0.0347       0.5286       \n",
      "best loss:     0.4319       1.2172       5.9241       6.7328       3.5765       \n",
      "best acc:      0.9046       0.8915       0.2834       0.0347       0.5286       \n",
      "\n",
      "total loss    3.5765        3.5765       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#JENS\n",
    "if True:\n",
    "    model_name = 'sideways cpo'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_sideways_graph_dict('conv_per_output')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, taxa, modified_transforms, seed, batch_size,\n",
    "                                                num_workers, device, 1, loss_coefs=None, root=custom_root,\n",
    "                                                criterion_constructor=WeightedBCELoss, reduction='sum', load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JENS\n",
    "if False:\n",
    "    batch_size = 32\n",
    "    model_name = 'fusion_forward recon no_disc explicit_encoder encoder_decoder_feedback'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {'graph_dict': fmd.make_forward_graph_dict_encoder_decoder_feedback('conv_per_output')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, ['genus','species'], modified_transforms, seed, batch_size,\n",
    "                                    num_workers, device, num_epochs, loss_coefs=None, root=recon_root, \n",
    "                                    criterion_constructor=WeightedBCELoss, reduction='sum', scale_factor = 0.5, load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHRISTIAN\n",
    "if False:\n",
    "    batch_size = 32\n",
    "    model_name = 'fusion_forward recon no_disc explicit_encoder spatial bidirectional_decoder add'\n",
    "    model_constructor = models.FusionModel\n",
    "    #base_model = torchvision.models.resnet18(False)\n",
    "    aux_model_args = {\n",
    "        'graph_dict': fmd.make_forward_graph_dict_bidirectional_decoder_spatial('add')}\n",
    "    #aux_scheduler_args = {'gamma': 1.0}\n",
    "\n",
    "    model, metrics = weighted_pipeline_tb_recon(model_constructor, aux_model_args, model_name, ['genus', 'species'], modified_transforms,\n",
    "                                                seed, batch_size, num_workers, device, num_epochs, loss_coefs=None,\n",
    "                                                root=recon_root, criterion_constructor=WeightedBCELoss,\n",
    "                                                reduction='sum', scale_factor=0.5, load_state_dict=False)\n",
    "    model.to('cpu')\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_name = 'fusion_forward recon no_disc explicit_encoder flat decoder_bidirectional'\n",
    "    one_shot(model_name,  recon_root,  ['genus', 'species'],\n",
    "            device, default_transforms, 'test_images/cyberwhoppee.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
